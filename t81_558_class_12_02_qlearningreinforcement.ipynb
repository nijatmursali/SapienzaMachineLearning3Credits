{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 12.2: Introduction to Q-Learning\n",
    "\n",
    "### Single Action Cart\n",
    "\n",
    "Mountain car actions:\n",
    "\n",
    "* 0 - Apply left force\n",
    "* 1 - Apply no force\n",
    "* 2 - Apply right force\n",
    "\n",
    "State values:\n",
    "\n",
    "* state[0] - Position \n",
    "* state[1] - Velocity\n",
    "\n",
    "The following shows a cart that simply applies full-force to climb the hill.  The cart is simply not strong enough.  It will need to use momentum from the hill behind it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 1, State: [-0.47707696  0.0006571 ], Reward = -1.0\n",
      "Step: 2, State: [-0.47576764  0.00130932], Reward = -1.0\n",
      "Step: 3, State: [-0.47381583  0.00195181], Reward = -1.0\n",
      "Step: 4, State: [-0.471236    0.00257983], Reward = -1.0\n",
      "Step: 5, State: [-0.46804728  0.00318872], Reward = -1.0\n",
      "Step: 6, State: [-0.46427327  0.00377401], Reward = -1.0\n",
      "Step: 7, State: [-0.45994186  0.00433141], Reward = -1.0\n",
      "Step: 8, State: [-0.45508497  0.00485688], Reward = -1.0\n",
      "Step: 9, State: [-0.44973833  0.00534664], Reward = -1.0\n",
      "Step: 10, State: [-0.44394112  0.00579721], Reward = -1.0\n",
      "Step: 11, State: [-0.43773568  0.00620545], Reward = -1.0\n",
      "Step: 12, State: [-0.43116711  0.00656857], Reward = -1.0\n",
      "Step: 13, State: [-0.42428292  0.00688418], Reward = -1.0\n",
      "Step: 14, State: [-0.41713263  0.00715029], Reward = -1.0\n",
      "Step: 15, State: [-0.40976734  0.0073653 ], Reward = -1.0\n",
      "Step: 16, State: [-0.40223928  0.00752806], Reward = -1.0\n",
      "Step: 17, State: [-0.39460144  0.00763784], Reward = -1.0\n",
      "Step: 18, State: [-0.38690711  0.00769433], Reward = -1.0\n",
      "Step: 19, State: [-0.37920948  0.00769763], Reward = -1.0\n",
      "Step: 20, State: [-0.37156122  0.00764826], Reward = -1.0\n",
      "Step: 21, State: [-0.36401411  0.00754711], Reward = -1.0\n",
      "Step: 22, State: [-0.35661869  0.00739542], Reward = -1.0\n",
      "Step: 23, State: [-0.34942389  0.0071948 ], Reward = -1.0\n",
      "Step: 24, State: [-0.34247677  0.00694712], Reward = -1.0\n",
      "Step: 25, State: [-0.33582219  0.00665457], Reward = -1.0\n",
      "Step: 26, State: [-0.32950263  0.00631956], Reward = -1.0\n",
      "Step: 27, State: [-0.32355791  0.00594472], Reward = -1.0\n",
      "Step: 28, State: [-0.31802505  0.00553286], Reward = -1.0\n",
      "Step: 29, State: [-0.3129381   0.00508695], Reward = -1.0\n",
      "Step: 30, State: [-0.30832801  0.00461009], Reward = -1.0\n",
      "Step: 31, State: [-0.30422253  0.00410547], Reward = -1.0\n",
      "Step: 32, State: [-0.30064616  0.00357638], Reward = -1.0\n",
      "Step: 33, State: [-0.29762001  0.00302615], Reward = -1.0\n",
      "Step: 34, State: [-0.29516182  0.00245818], Reward = -1.0\n",
      "Step: 35, State: [-0.29328592  0.0018759 ], Reward = -1.0\n",
      "Step: 36, State: [-0.29200317  0.00128275], Reward = -1.0\n",
      "Step: 37, State: [-0.29132098  0.00068219], Reward = -1.0\n",
      "Step: 38, State: [-2.91243266e-01  7.77126097e-05], Reward = -1.0\n",
      "Step: 39, State: [-0.29177048 -0.00052722], Reward = -1.0\n",
      "Step: 40, State: [-0.29289959 -0.00112911], Reward = -1.0\n",
      "Step: 41, State: [-0.29462409 -0.00172449], Reward = -1.0\n",
      "Step: 42, State: [-0.29693398 -0.0023099 ], Reward = -1.0\n",
      "Step: 43, State: [-0.29981585 -0.00288187], Reward = -1.0\n",
      "Step: 44, State: [-0.30325283 -0.00343698], Reward = -1.0\n",
      "Step: 45, State: [-0.30722465 -0.00397182], Reward = -1.0\n",
      "Step: 46, State: [-0.31170769 -0.00448304], Reward = -1.0\n",
      "Step: 47, State: [-0.31667502 -0.00496733], Reward = -1.0\n",
      "Step: 48, State: [-0.32209651 -0.00542149], Reward = -1.0\n",
      "Step: 49, State: [-0.32793889 -0.00584238], Reward = -1.0\n",
      "Step: 50, State: [-0.3341659 -0.006227 ], Reward = -1.0\n",
      "Step: 51, State: [-0.3407384 -0.0065725], Reward = -1.0\n",
      "Step: 52, State: [-0.34761459 -0.00687619], Reward = -1.0\n",
      "Step: 53, State: [-0.3547502  -0.00713561], Reward = -1.0\n",
      "Step: 54, State: [-0.36209871 -0.00734851], Reward = -1.0\n",
      "Step: 55, State: [-0.36961163 -0.00751292], Reward = -1.0\n",
      "Step: 56, State: [-0.37723882 -0.00762719], Reward = -1.0\n",
      "Step: 57, State: [-0.38492877 -0.00768995], Reward = -1.0\n",
      "Step: 58, State: [-0.39262901 -0.00770024], Reward = -1.0\n",
      "Step: 59, State: [-0.40028644 -0.00765743], Reward = -1.0\n",
      "Step: 60, State: [-0.40784776 -0.00756132], Reward = -1.0\n",
      "Step: 61, State: [-0.41525988 -0.00741211], Reward = -1.0\n",
      "Step: 62, State: [-0.4224703  -0.00721042], Reward = -1.0\n",
      "Step: 63, State: [-0.42942761 -0.00695731], Reward = -1.0\n",
      "Step: 64, State: [-0.43608184 -0.00665423], Reward = -1.0\n",
      "Step: 65, State: [-0.44238493 -0.00630309], Reward = -1.0\n",
      "Step: 66, State: [-0.44829112 -0.00590619], Reward = -1.0\n",
      "Step: 67, State: [-0.45375733 -0.0054662 ], Reward = -1.0\n",
      "Step: 68, State: [-0.45874352 -0.00498619], Reward = -1.0\n",
      "Step: 69, State: [-0.46321306 -0.00446954], Reward = -1.0\n",
      "Step: 70, State: [-0.46713303 -0.00391996], Reward = -1.0\n",
      "Step: 71, State: [-0.47047446 -0.00334143], Reward = -1.0\n",
      "Step: 72, State: [-0.47321264 -0.00273818], Reward = -1.0\n",
      "Step: 73, State: [-0.47532728 -0.00211464], Reward = -1.0\n",
      "Step: 74, State: [-0.47680269 -0.00147541], Reward = -1.0\n",
      "Step: 75, State: [-0.47762792 -0.00082523], Reward = -1.0\n",
      "Step: 76, State: [-4.77796842e-01 -1.68920140e-04], Reward = -1.0\n",
      "Step: 77, State: [-0.4773082   0.00048865], Reward = -1.0\n",
      "Step: 78, State: [-0.47616562  0.00114258], Reward = -1.0\n",
      "Step: 79, State: [-0.47437758  0.00178803], Reward = -1.0\n",
      "Step: 80, State: [-0.47195737  0.00242021], Reward = -1.0\n",
      "Step: 81, State: [-0.46892292  0.00303445], Reward = -1.0\n",
      "Step: 82, State: [-0.46529671  0.00362622], Reward = -1.0\n",
      "Step: 83, State: [-0.46110553  0.00419118], Reward = -1.0\n",
      "Step: 84, State: [-0.45638031  0.00472522], Reward = -1.0\n",
      "Step: 85, State: [-0.45115582  0.00522449], Reward = -1.0\n",
      "Step: 86, State: [-0.44547038  0.00568544], Reward = -1.0\n",
      "Step: 87, State: [-0.43936556  0.00610482], Reward = -1.0\n",
      "Step: 88, State: [-0.43288578  0.00647978], Reward = -1.0\n",
      "Step: 89, State: [-0.42607799  0.00680779], Reward = -1.0\n",
      "Step: 90, State: [-0.41899121  0.00708678], Reward = -1.0\n",
      "Step: 91, State: [-0.41167618  0.00731504], Reward = -1.0\n",
      "Step: 92, State: [-0.40418487  0.0074913 ], Reward = -1.0\n",
      "Step: 93, State: [-0.39657014  0.00761473], Reward = -1.0\n",
      "Step: 94, State: [-0.38888524  0.00768491], Reward = -1.0\n",
      "Step: 95, State: [-0.3811834   0.00770184], Reward = -1.0\n",
      "Step: 96, State: [-0.37351748  0.00766592], Reward = -1.0\n",
      "Step: 97, State: [-0.36593952  0.00757796], Reward = -1.0\n",
      "Step: 98, State: [-0.35850041  0.00743911], Reward = -1.0\n",
      "Step: 99, State: [-0.35124952  0.00725088], Reward = -1.0\n",
      "Step: 100, State: [-0.34423443  0.00701509], Reward = -1.0\n",
      "Step: 101, State: [-0.33750059  0.00673384], Reward = -1.0\n",
      "Step: 102, State: [-0.33109109  0.00640949], Reward = -1.0\n",
      "Step: 103, State: [-0.32504648  0.00604462], Reward = -1.0\n",
      "Step: 104, State: [-0.31940449  0.00564199], Reward = -1.0\n",
      "Step: 105, State: [-0.31419996  0.00520453], Reward = -1.0\n",
      "Step: 106, State: [-0.30946465  0.00473531], Reward = -1.0\n",
      "Step: 107, State: [-0.30522714  0.00423751], Reward = -1.0\n",
      "Step: 108, State: [-0.30151275  0.00371439], Reward = -1.0\n",
      "Step: 109, State: [-0.29834349  0.00316926], Reward = -1.0\n",
      "Step: 110, State: [-0.29573796  0.00260553], Reward = -1.0\n",
      "Step: 111, State: [-0.29371138  0.00202659], Reward = -1.0\n",
      "Step: 112, State: [-0.29227548  0.0014359 ], Reward = -1.0\n",
      "Step: 113, State: [-0.29143856  0.00083691], Reward = -1.0\n",
      "Step: 114, State: [-2.91205456e-01  2.33108360e-04], Reward = -1.0\n",
      "Step: 115, State: [-0.29157749 -0.00037204], Reward = -1.0\n",
      "Step: 116, State: [-0.29255254 -0.00097504], Reward = -1.0\n",
      "Step: 117, State: [-0.29412497 -0.00157243], Reward = -1.0\n",
      "Step: 118, State: [-0.29628569 -0.00216073], Reward = -1.0\n",
      "Step: 119, State: [-0.29902217 -0.00273648], Reward = -1.0\n",
      "Step: 120, State: [-0.30231841 -0.00329624], Reward = -1.0\n",
      "Step: 121, State: [-0.30615501 -0.00383661], Reward = -1.0\n",
      "Step: 122, State: [-0.31050922 -0.00435421], Reward = -1.0\n",
      "Step: 123, State: [-0.31535495 -0.00484573], Reward = -1.0\n",
      "Step: 124, State: [-0.32066288 -0.00530793], Reward = -1.0\n",
      "Step: 125, State: [-0.32640053 -0.00573765], Reward = -1.0\n",
      "Step: 126, State: [-0.3325324  -0.00613187], Reward = -1.0\n",
      "Step: 127, State: [-0.33902007 -0.00648767], Reward = -1.0\n",
      "Step: 128, State: [-0.34582242 -0.00680234], Reward = -1.0\n",
      "Step: 129, State: [-0.35289577 -0.00707335], Reward = -1.0\n",
      "Step: 130, State: [-0.36019416 -0.00729839], Reward = -1.0\n",
      "Step: 131, State: [-0.36766959 -0.00747543], Reward = -1.0\n",
      "Step: 132, State: [-0.3752723  -0.00760271], Reward = -1.0\n",
      "Step: 133, State: [-0.38295111 -0.00767881], Reward = -1.0\n",
      "Step: 134, State: [-0.39065376 -0.00770264], Reward = -1.0\n",
      "Step: 135, State: [-0.39832726 -0.00767351], Reward = -1.0\n",
      "Step: 136, State: [-0.40591835 -0.00759108], Reward = -1.0\n",
      "Step: 137, State: [-0.41337381 -0.00745547], Reward = -1.0\n",
      "Step: 138, State: [-0.42064098 -0.00726717], Reward = -1.0\n",
      "Step: 139, State: [-0.42766812 -0.00702713], Reward = -1.0\n",
      "Step: 140, State: [-0.43440484 -0.00673672], Reward = -1.0\n",
      "Step: 141, State: [-0.44080256 -0.00639772], Reward = -1.0\n",
      "Step: 142, State: [-0.44681489 -0.00601233], Reward = -1.0\n",
      "Step: 143, State: [-0.45239802 -0.00558313], Reward = -1.0\n",
      "Step: 144, State: [-0.4575111  -0.00511308], Reward = -1.0\n",
      "Step: 145, State: [-0.4621166 -0.0046055], Reward = -1.0\n",
      "Step: 146, State: [-0.46618061 -0.00406401], Reward = -1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 147, State: [-0.46967312 -0.00349252], Reward = -1.0\n",
      "Step: 148, State: [-0.47256832 -0.0028952 ], Reward = -1.0\n",
      "Step: 149, State: [-0.47484475 -0.00227643], Reward = -1.0\n",
      "Step: 150, State: [-0.47648553 -0.00164078], Reward = -1.0\n",
      "Step: 151, State: [-0.47747849 -0.00099296], Reward = -1.0\n",
      "Step: 152, State: [-4.77816248e-01 -3.37757488e-04], Reward = -1.0\n",
      "Step: 153, State: [-4.77496296e-01  3.19952109e-04], Reward = -1.0\n",
      "Step: 154, State: [-0.47652101  0.00097528], Reward = -1.0\n",
      "Step: 155, State: [-0.47489764  0.00162337], Reward = -1.0\n",
      "Step: 156, State: [-0.47263822  0.00225941], Reward = -1.0\n",
      "Step: 157, State: [-0.46975953  0.0028787 ], Reward = -1.0\n",
      "Step: 158, State: [-0.46628287  0.00347666], Reward = -1.0\n",
      "Step: 159, State: [-0.46223397  0.0040489 ], Reward = -1.0\n",
      "Step: 160, State: [-0.45764271  0.00459126], Reward = -1.0\n",
      "Step: 161, State: [-0.4525429   0.00509981], Reward = -1.0\n",
      "Step: 162, State: [-0.44697198  0.00557092], Reward = -1.0\n",
      "Step: 163, State: [-0.44097071  0.00600127], Reward = -1.0\n",
      "Step: 164, State: [-0.43458283  0.00638788], Reward = -1.0\n",
      "Step: 165, State: [-0.42785466  0.00672817], Reward = -1.0\n",
      "Step: 166, State: [-0.42083473  0.00701992], Reward = -1.0\n",
      "Step: 167, State: [-0.41357339  0.00726134], Reward = -1.0\n",
      "Step: 168, State: [-0.40612233  0.00745106], Reward = -1.0\n",
      "Step: 169, State: [-0.39853422  0.00758811], Reward = -1.0\n",
      "Step: 170, State: [-0.39086225  0.00767198], Reward = -1.0\n",
      "Step: 171, State: [-0.38315969  0.00770256], Reward = -1.0\n",
      "Step: 172, State: [-0.37547954  0.00768015], Reward = -1.0\n",
      "Step: 173, State: [-0.36787409  0.00760545], Reward = -1.0\n",
      "Step: 174, State: [-0.36039455  0.00747954], Reward = -1.0\n",
      "Step: 175, State: [-0.35309072  0.00730383], Reward = -1.0\n",
      "Step: 176, State: [-0.34601065  0.00708006], Reward = -1.0\n",
      "Step: 177, State: [-0.33920038  0.00681027], Reward = -1.0\n",
      "Step: 178, State: [-0.33270363  0.00649675], Reward = -1.0\n",
      "Step: 179, State: [-0.32656161  0.00614202], Reward = -1.0\n",
      "Step: 180, State: [-0.32081279  0.00574881], Reward = -1.0\n",
      "Step: 181, State: [-0.31549278  0.00532001], Reward = -1.0\n",
      "Step: 182, State: [-0.31063413  0.00485865], Reward = -1.0\n",
      "Step: 183, State: [-0.30626625  0.00436788], Reward = -1.0\n",
      "Step: 184, State: [-0.30241531  0.00385094], Reward = -1.0\n",
      "Step: 185, State: [-0.29910416  0.00331115], Reward = -1.0\n",
      "Step: 186, State: [-0.2963523   0.00275187], Reward = -1.0\n",
      "Step: 187, State: [-0.29417579  0.0021765 ], Reward = -1.0\n",
      "Step: 188, State: [-0.29258729  0.0015885 ], Reward = -1.0\n",
      "Step: 189, State: [-0.29159598  0.00099131], Reward = -1.0\n",
      "Step: 190, State: [-0.29120756  0.00038842], Reward = -1.0\n",
      "Step: 191, State: [-2.91424279e-01 -2.16717796e-04], Reward = -1.0\n",
      "Step: 192, State: [-0.29224488 -0.0008206 ], Reward = -1.0\n",
      "Step: 193, State: [-0.29366465 -0.00141977], Reward = -1.0\n",
      "Step: 194, State: [-0.29567538 -0.00201073], Reward = -1.0\n",
      "Step: 195, State: [-0.29826541 -0.00259003], Reward = -1.0\n",
      "Step: 196, State: [-0.30141963 -0.00315422], Reward = -1.0\n",
      "Step: 197, State: [-0.30511952 -0.00369989], Reward = -1.0\n",
      "Step: 198, State: [-0.30934318 -0.00422366], Reward = -1.0\n",
      "Step: 199, State: [-0.31406537 -0.00472219], Reward = -1.0\n",
      "Step: 200, State: [-0.31925759 -0.00519222], Reward = -1.0\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "env = gym.make(\"MountainCar-v0\")\n",
    "env.reset()\n",
    "done = False\n",
    "\n",
    "i = 0\n",
    "while not done:\n",
    "    i += 1\n",
    "    state, reward, done, _ = env.step(2)\n",
    "    env.render()\n",
    "    #print(f\"Step {i}: State={state}, Reward={reward}\")\n",
    "    print(\"Step: {0}, State: {1}, Reward = {2}\".format(i, state,reward))\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Programmed Car\n",
    "\n",
    "This is a car that I hand-programmed.  It uses a simple rule, but solves the problem. The programmed car constantly applies force to one direction or another.  It does not reset.  Whatever direction the car is currently rolling, it applies force in that direction.  Therefore, the car begins to climb a hill, is overpowered, and rolls backward.  However, once it begins to roll backwards force is immediately applied in this new direction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 1, State: [-0.441596   -0.00162062], Reward = -1.0\n",
      "Step: 2, State: [-0.44482546 -0.00322945], Reward = -1.0\n",
      "Step: 3, State: [-0.44964022 -0.00481477], Reward = -1.0\n",
      "Step: 4, State: [-0.45600514 -0.00636492], Reward = -1.0\n",
      "Step: 5, State: [-0.46387355 -0.0078684 ], Reward = -1.0\n",
      "Step: 6, State: [-0.4731875  -0.00931395], Reward = -1.0\n",
      "Step: 7, State: [-0.48387809 -0.0106906 ], Reward = -1.0\n",
      "Step: 8, State: [-0.49586589 -0.0119878 ], Reward = -1.0\n",
      "Step: 9, State: [-0.50906144 -0.01319555], Reward = -1.0\n",
      "Step: 10, State: [-0.52336599 -0.01430455], Reward = -1.0\n",
      "Step: 11, State: [-0.53867228 -0.01530629], Reward = -1.0\n",
      "Step: 12, State: [-0.55486556 -0.01619328], Reward = -1.0\n",
      "Step: 13, State: [-0.57182469 -0.01695912], Reward = -1.0\n",
      "Step: 14, State: [-0.58942338 -0.01759869], Reward = -1.0\n",
      "Step: 15, State: [-0.60753159 -0.01810821], Reward = -1.0\n",
      "Step: 16, State: [-0.62601693 -0.01848534], Reward = -1.0\n",
      "Step: 17, State: [-0.64474616 -0.01872924], Reward = -1.0\n",
      "Step: 18, State: [-0.66358667 -0.0188405 ], Reward = -1.0\n",
      "Step: 19, State: [-0.68240785 -0.01882118], Reward = -1.0\n",
      "Step: 20, State: [-0.70108251 -0.01867467], Reward = -1.0\n",
      "Step: 21, State: [-0.71948806 -0.01840555], Reward = -1.0\n",
      "Step: 22, State: [-0.73750756 -0.01801949], Reward = -1.0\n",
      "Step: 23, State: [-0.7550306  -0.01752305], Reward = -1.0\n",
      "Step: 24, State: [-0.77195404 -0.01692344], Reward = -1.0\n",
      "Step: 25, State: [-0.78818243 -0.01622839], Reward = -1.0\n",
      "Step: 26, State: [-0.80362834 -0.01544591], Reward = -1.0\n",
      "Step: 27, State: [-0.8182125  -0.01458416], Reward = -1.0\n",
      "Step: 28, State: [-0.83186371 -0.01365121], Reward = -1.0\n",
      "Step: 29, State: [-0.84451867 -0.01265496], Reward = -1.0\n",
      "Step: 30, State: [-0.85612171 -0.01160304], Reward = -1.0\n",
      "Step: 31, State: [-0.86662435 -0.01050265], Reward = -1.0\n",
      "Step: 32, State: [-0.87598495 -0.00936059], Reward = -1.0\n",
      "Step: 33, State: [-0.88416813 -0.00818318], Reward = -1.0\n",
      "Step: 34, State: [-0.89114441 -0.00697628], Reward = -1.0\n",
      "Step: 35, State: [-0.89688969 -0.00574528], Reward = -1.0\n",
      "Step: 36, State: [-0.90138485 -0.00449517], Reward = -1.0\n",
      "Step: 37, State: [-0.90461542 -0.00323057], Reward = -1.0\n",
      "Step: 38, State: [-0.90657123 -0.00195581], Reward = -1.0\n",
      "Step: 39, State: [-9.07246235e-01 -6.75006284e-04], Reward = -1.0\n",
      "Step: 40, State: [-9.06638370e-01  6.07864927e-04], Reward = -1.0\n",
      "Step: 41, State: [-0.9027495   0.00388887], Reward = -1.0\n",
      "Step: 42, State: [-0.89559171  0.00715779], Reward = -1.0\n",
      "Step: 43, State: [-0.88518806  0.01040364], Reward = -1.0\n",
      "Step: 44, State: [-0.87157393  0.01361413], Reward = -1.0\n",
      "Step: 45, State: [-0.85479884  0.01677509], Reward = -1.0\n",
      "Step: 46, State: [-0.83492876  0.01987008], Reward = -1.0\n",
      "Step: 47, State: [-0.81204868  0.02288008], Reward = -1.0\n",
      "Step: 48, State: [-0.78626529  0.02578338], Reward = -1.0\n",
      "Step: 49, State: [-0.75770955  0.02855574], Reward = -1.0\n",
      "Step: 50, State: [-0.7265388   0.03117074], Reward = -1.0\n",
      "Step: 51, State: [-0.69293831  0.03360049], Reward = -1.0\n",
      "Step: 52, State: [-0.6571217   0.03581661], Reward = -1.0\n",
      "Step: 53, State: [-0.61933023  0.03779147], Reward = -1.0\n",
      "Step: 54, State: [-0.57983061  0.03949962], Reward = -1.0\n",
      "Step: 55, State: [-0.53891124  0.04091936], Reward = -1.0\n",
      "Step: 56, State: [-0.49687707  0.04203417], Reward = -1.0\n",
      "Step: 57, State: [-0.45404311  0.04283397], Reward = -1.0\n",
      "Step: 58, State: [-0.41072703  0.04331608], Reward = -1.0\n",
      "Step: 59, State: [-0.3672414   0.04348563], Reward = -1.0\n",
      "Step: 60, State: [-0.32388592  0.04335548], Reward = -1.0\n",
      "Step: 61, State: [-0.28094027  0.04294565], Reward = -1.0\n",
      "Step: 62, State: [-0.23865802  0.04228225], Reward = -1.0\n",
      "Step: 63, State: [-0.1972619   0.04139612], Reward = -1.0\n",
      "Step: 64, State: [-0.15694065  0.04032125], Reward = -1.0\n",
      "Step: 65, State: [-0.11784739  0.03909326], Reward = -1.0\n",
      "Step: 66, State: [-0.08009951  0.03774788], Reward = -1.0\n",
      "Step: 67, State: [-0.04377979  0.03631971], Reward = -1.0\n",
      "Step: 68, State: [-0.00893855  0.03484125], Reward = -1.0\n",
      "Step: 69, State: [0.0244036  0.03334214], Reward = -1.0\n",
      "Step: 70, State: [0.05625244 0.03184884], Reward = -1.0\n",
      "Step: 71, State: [0.0866368  0.03038436], Reward = -1.0\n",
      "Step: 72, State: [0.11560512 0.02896832], Reward = -1.0\n",
      "Step: 73, State: [0.14322229 0.02761717], Reward = -1.0\n",
      "Step: 74, State: [0.1695667  0.02634441], Reward = -1.0\n",
      "Step: 75, State: [0.19472767 0.02516097], Reward = -1.0\n",
      "Step: 76, State: [0.21880323 0.02407556], Reward = -1.0\n",
      "Step: 77, State: [0.24189832 0.02309509], Reward = -1.0\n",
      "Step: 78, State: [0.26412331 0.02222499], Reward = -1.0\n",
      "Step: 79, State: [0.2855929  0.02146959], Reward = -1.0\n",
      "Step: 80, State: [0.3064253 0.0208324], Reward = -1.0\n",
      "Step: 81, State: [0.32674172 0.02031641], Reward = -1.0\n",
      "Step: 82, State: [0.34666604 0.01992432], Reward = -1.0\n",
      "Step: 83, State: [0.36632481 0.01965877], Reward = -1.0\n",
      "Step: 84, State: [0.38584731 0.0195225 ], Reward = -1.0\n",
      "Step: 85, State: [0.40536582 0.01951852], Reward = -1.0\n",
      "Step: 86, State: [0.42501607 0.01965025], Reward = -1.0\n",
      "Step: 87, State: [0.44493767 0.01992161], Reward = -1.0\n",
      "Step: 88, State: [0.46527478 0.02033711], Reward = -1.0\n",
      "Step: 89, State: [0.48617669 0.02090191], Reward = -1.0\n",
      "Step: 90, State: [0.50779852 0.02162183], Reward = -1.0\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "env = gym.make(\"MountainCar-v0\")\n",
    "state = env.reset()\n",
    "done = False\n",
    "\n",
    "i = 0\n",
    "while not done:\n",
    "    i += 1\n",
    "    \n",
    "    if state[1]>0:\n",
    "        action = 2\n",
    "    else:\n",
    "        action = 0\n",
    "    \n",
    "    state, reward, done, _ = env.step(action)\n",
    "    env.render()\n",
    "    #print(f\"Step {i}: State={state}, Reward={reward}\")\n",
    "    print(\"Step: {0}, State: {1}, Reward = {2}\".format(i, state,reward))\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reinforcement Learning\n",
    "\n",
    "![Reinforcement Learning](https://raw.githubusercontent.com/jeffheaton/t81_558_deep_learning/master/images/reinforcement.png \"Reinforcement Learning\")\n",
    "\n",
    "\n",
    "### Q-Learning Car\n",
    "\n",
    "We will now use Q-Learning to produce a car that learns to drive itself.  Look out Tesla! \n",
    "\n",
    "Q-Learning works by building a table that provides a lookup table to determine which of several actions should be taken. As we move through a number of training episodes this table is refined.\n",
    "\n",
    "$ Q^{new}(s_{t},a_{t}) \\leftarrow (1-\\alpha) \\cdot \\underbrace{Q(s_{t},a_{t})}_{\\text{old value}} + \\underbrace{\\alpha}_{\\text{learning rate}} \\cdot  \\overbrace{\\bigg( \\underbrace{r_{t}}_{\\text{reward}} + \\underbrace{\\gamma}_{\\text{discount factor}} \\cdot \\underbrace{\\max_{a}Q(s_{t+1}, a)}_{\\text{estimate of optimal future value}} \\bigg) }^{\\text{learned value}} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "def calc_discrete_state(state):\n",
    "    discrete_state = (state - env.observation_space.low)/buckets\n",
    "    return tuple(discrete_state.astype(np.int))  \n",
    "\n",
    "def run_game(q_table, render, should_update):\n",
    "    done = False\n",
    "    discrete_state = calc_discrete_state(env.reset())\n",
    "    success = False\n",
    "    \n",
    "    while not done:\n",
    "        # Exploit or explore\n",
    "        if np.random.random() > epsilon:\n",
    "            # Exploit - use q-table to take current best action (and probably refine)\n",
    "            action = np.argmax(q_table[discrete_state])\n",
    "        else:\n",
    "            # Explore - t\n",
    "            action = np.random.randint(0, env.action_space.n)\n",
    "            \n",
    "        # Run simulation step\n",
    "        new_state, reward, done, _ = env.step(action)\n",
    "        \n",
    "        # \n",
    "        new_state_disc = calc_discrete_state(new_state)\n",
    "\n",
    "        # \n",
    "        if new_state[0] >= env.goal_position:\n",
    "            success = True\n",
    "          \n",
    "        # Update q-table\n",
    "        if should_update:\n",
    "            max_future_q = np.max(q_table[new_state_disc])\n",
    "            current_q = q_table[discrete_state + (action,)]\n",
    "            new_q = (1 - LEARNING_RATE) * current_q + LEARNING_RATE * (reward + DISCOUNT * max_future_q)\n",
    "            q_table[discrete_state + (action,)] = new_q\n",
    "\n",
    "        discrete_state = new_state_disc\n",
    "        \n",
    "        if render:\n",
    "            env.render()\n",
    "            \n",
    "    return success\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.1\n",
    "DISCOUNT = 0.95\n",
    "EPISODES = 200\n",
    "SHOW_EVERY = 40\n",
    "\n",
    "DISCRETE_GRID_SIZE = [10, 10]\n",
    "START_EPSILON_DECAYING = 1\n",
    "END_EPSILON_DECAYING = EPISODES//2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "env = gym.make(\"MountainCar-v0\")\n",
    "\n",
    "epsilon = 1  \n",
    "epsilon_change = epsilon/(END_EPSILON_DECAYING - START_EPSILON_DECAYING)\n",
    "buckets = (env.observation_space.high - env.observation_space.low)/DISCRETE_GRID_SIZE\n",
    "q_table = np.random.uniform(low=-3, high=0, size=(DISCRETE_GRID_SIZE + [env.action_space.n]))\n",
    "success = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current episode: 40, success: 0\n",
      "Current episode: 80, success: 0\n",
      "Current episode: 120, success: 0\n",
      "Current episode: 160, success: 1\n",
      "Current episode: 200, success: 3\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "episode = 0\n",
    "success_count = 0\n",
    "\n",
    "while episode<EPISODES:\n",
    "    episode+=1\n",
    "    done = False\n",
    "\n",
    "    if episode % SHOW_EVERY == 0:\n",
    "        #print(f\"Current episode: {episode}, success: {success_count} ({float(success_count)/SHOW_EVERY})\")\n",
    "        print(\"Current episode: {0}, success: {1}\".format(episode, success_count,float(success_count)/SHOW_EVERY))\n",
    "        success = run_game(q_table, True, False)\n",
    "        success_count = 0\n",
    "    else:\n",
    "        success = run_game(q_table, False, True)\n",
    "        \n",
    "    if success:\n",
    "        success_count += 1\n",
    "\n",
    "    # Move epsilon towards its ending value, if it still needs to move\n",
    "    if END_EPSILON_DECAYING >= episode >= START_EPSILON_DECAYING:\n",
    "        epsilon -= epsilon_change\n",
    "\n",
    "print(success)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_game(q_table, True, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close() #end the process "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(q_table.argmax(axis=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v-{0}</th>\n",
       "      <th>v-{0}</th>\n",
       "      <th>v-{0}</th>\n",
       "      <th>v-{0}</th>\n",
       "      <th>v-{0}</th>\n",
       "      <th>v-{0}</th>\n",
       "      <th>v-{0}</th>\n",
       "      <th>v-{0}</th>\n",
       "      <th>v-{0}</th>\n",
       "      <th>v-{0}</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>p-{1}</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>p-{1}</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>p-{1}</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>p-{1}</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>p-{1}</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>p-{1}</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>p-{1}</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>p-{1}</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>p-{1}</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>p-{1}</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       v-{0}  v-{0}  v-{0}  v-{0}  v-{0}  v-{0}  v-{0}  v-{0}  v-{0}  v-{0}\n",
       "p-{1}      2      2      0      0      2      1      0      0      2      2\n",
       "p-{1}      0      1      0      1      2      0      1      2      1      1\n",
       "p-{1}      1      1      1      0      2      2      1      2      1      0\n",
       "p-{1}      1      0      0      0      0      2      2      1      0      1\n",
       "p-{1}      0      0      0      0      0      0      2      2      2      2\n",
       "p-{1}      2      2      0      1      0      2      2      1      1      1\n",
       "p-{1}      1      2      0      1      2      0      1      1      0      2\n",
       "p-{1}      1      1      2      1      2      2      0      1      2      2\n",
       "p-{1}      0      1      2      0      1      2      1      1      1      1\n",
       "p-{1}      1      2      2      2      2      2      2      2      2      2"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns = ['v-{0}' for x in range(DISCRETE_GRID_SIZE[0])]\n",
    "df.index = ['p-{1}' for x in range(DISCRETE_GRID_SIZE[1])]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "np.argmax(q_table[(2,0)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
